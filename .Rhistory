#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0 # effect size is 0: no difference between arms
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
N <- 15 # sample_size(total number of trials in one simulation)
B <- 5000 # number of simulations
c <- 0.1 # threshold
burn_in <- 2 # # of burnin trials
n <-  3 # batch_size
Ti <- N/n # num_batches
results <- array(dim = c(B, 4))
for (i in 1:B) {
results[i,] <-
TSPDD(
pa = p0,
pb = p1,
n = Ti,
c = c,
burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1, burnin=2 means there is one UR distribution before apply TSPost-Diff
batch = n
)
}
average_power = mean(results[, 1] > 1.96)
average_reward = mean(results[, 2])
APT1 = results[,3]
APT2 = results[,4]
#Histogram of APT1
hist(APT1, freq = FALSE)
# empirical and normal CV of APT1 is
em_cv <- empirical_cv(APT1,alpha)
paste('empirical CV is', em_cv)
norm_cv <- normal_cv(APT1,alpha)
paste('normal CV is', norm_cv)
# empirical cdf of APT1 is
F1 <- ecdf(APT1)
# empirical and normal FPR of APT1 is
paste('empirical FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
knitr::opts_chunk$set(echo = TRUE)
#' TSPostdiff AP Test
#'
#' Runs TSPostdiff in two armed scenario for n batches, with given batch size and burnin period, calculate AP Test statistic with or without UR.
#' @param pa prior probability of arm a
#' @param pb prior probability of arm b
#' @param n number of trials in one simulation
#' @param burnin number of batches that runs only uniform random sampling before applying TS-Post Diff to provide more successes and failures in the beta distribution # nolint
#' @param batch number of trials before updating posterior probability
#'
#' @return vector containing WaldScore[n], reward[n], APT_ban_UR, APT_w_UR: the final waldscore, reward, and AP test statistics for both arms
TSPDD <- function(pa, pb, n, c, burnin, batch) {
# Set arm success and failures
# ex. [0,1,1,1,2, 3, 3, 4, 5, 6] final index is total successes
arm_a_successes <- c(0)
arm_a_failures <- c(0)
arm_b_successes <- c(0)
arm_b_failures <- c(0)
# Successes and failures as a list of one number at a certain time
arm_a_s <- c(0)
arm_a_f <- c(0)
arm_b_s <- c(0)
arm_b_f <- c(0)
# AP Test statistic for arm a and arm b
APT_ban_UR = 0
APT_w_UR = 0
# list that tracks number of successes for all arms
# history generated for all the arms successes and failures
abl <- c()
# BURNIN PERIOD: use UR only
for (i in 1:burnin) {
# draw_a and draw_b are local variables, determine which arm add the first UR distribution
# draw a and draw b: allocation probability of arms a and b
draw_a <- runif(1)
draw_b <- runif(1) #whenever runif(1) < pa/pb is indicated inside of draw a
#and draw b comparisons, it means an arm is being
#pulled (with random probability)
# if draw a is bigger than draw b
if (draw_a > draw_b) {
# "choose arm a"
# Check if smaller than prior probability, update abl, number of successes & failures for arm a
# new randomly generated probability is smaller than the prior probability
# check if arm a was "successful", has reward
if (runif(1) < pa) {
# arm a is successful
abl[i] <- 1
arm_a_s <- arm_a_s + 1  #runif(1) is different in each pa/pb comparison
} else {
# arm a was not successful (not chosen)
abl[i] <- 0
arm_a_f <- arm_a_f + 1
}
}
# if draw b is bigger than draw a
else{
# Check if smaller than prior probability, update abl, number of successes & failures for arm b
if (runif(1) < pb) {
abl[i] <- 1
arm_b_s <- arm_b_s + 1
}
else {
abl[i] <- 0
arm_b_f <- arm_b_f + 1
}
}
# Update total number of success and failures for arms a and b
# TODO: why not directly add to arm_a_successes earlier? Difference between these
arm_a_successes[i] <- arm_a_s
arm_a_failures[i] <- arm_a_f
arm_b_successes[i] <- arm_b_s
arm_b_failures[i] <- arm_b_f
#^^tracking the build-up of successes and failures under each arm, during
#each iteration of for loop, starts at 0, continues until #burnin                    #total iterations, first index = 0, last index = # of successes
}
# NOT BURNIN PERIOD: Use TS or UR depending on difference between posterior probabilities
for (i in (burnin + 1):n) {
# update posterior probability based on batch size
m = i %% batch
# Update posterior probability
if (m == 0){
draw_a <-
rbeta(1, arm_a_successes[i - 1] + 1, arm_a_failures[i - 1] + 1) #draw from Beta(alpha, beta) distribution based on current successes & failures
draw_b <-
rbeta(1, arm_b_successes[i - 1] + 1, arm_b_failures[i - 1] + 1) # alpha = current successes, beta = current failures of each arm
}
# Does not update posterior probability
else{
draw_a <-
rbeta(1, arm_a_successes[i - m] + 1, arm_a_failures[i - m] + 1) # using the last settings for the distribution (not updated yet)
draw_b <-
rbeta(1, arm_b_successes[i - m] + 1, arm_b_failures[i - m] + 1)
}
# you can change 1000 to 10000 or whatever to make the result better.
# if difference between the posterior probabilities is smaller than the threshold, use UR
if (abs(draw_a - draw_b) < c) {
draw_a <- runif(1)
draw_b <- runif(1)
}
# otherwise, use TS
# TODO: refactor code for TS and UR? avoid repetitive code
# generate TS based on past history
else {
if (m == 0){ #batch = # of trials iterated before posterior prob update
#this means whenever m becomes 0 again, the post prob
#is updated over and over again
#greater batch size -> slower and less post prob updates
draw_a <-
rbeta(1, arm_a_successes[i - 1] + 1, arm_a_failures[i - 1] + 1)
draw_b <-
rbeta(1, arm_b_successes[i - 1] + 1, arm_b_failures[i - 1] + 1)
}
#each time batch size is "crossed", m == 0, rbeta parameters are
#updated to include most current success/failure counts, post prob
#when m != 0, we stick with info from previous batch, older post prob
#why rbeta? most suitable for binary reward system such as TSPDD
#you can change 1000 to 10000 or whatever to make the result better
else{
draw_a <-
rbeta(1, arm_a_successes[i - m] + 1, arm_a_failures[i - m] + 1)
draw_b <-
rbeta(1, arm_b_successes[i - m] + 1, arm_b_failures[i - m] + 1)
}
#same TS updating process done here again but on a more local level,
#if the effect size is better than the threshold, it shows we are
#willing to go back to exploiting again, when effect size is smaller,
#we are open to doing more exploration with UR
# update AP_{ban UR} (without UR sampling)
if (draw_b >draw_a){
APT_ban_UR = APT_ban_UR + 1
}
}
# Update arm a and arm b history
# Always runs, no matter if it is UR or TS
if (draw_a > draw_b) {
if (runif(1) < pa) {
abl[i] <- 1
arm_a_s <- arm_a_s + 1
} else {
abl[i] <- 0
arm_a_f <- arm_a_f + 1
}
} else{
if (runif(1) < pb) {
abl[i] <- 1
arm_b_s <- arm_b_s + 1
} else {
abl[i] <- 0
arm_b_f <- arm_b_f + 1
}
}
# AP_{w/ UR} (including the ones with UR)
if (draw_b > draw_a){
APT_w_UR <- APT_w_UR + 1
}
arm_a_successes[i] <- arm_a_s
arm_a_failures[i] <- arm_a_f
arm_b_successes[i] <- arm_b_s
arm_b_failures[i] <- arm_b_f
}
#na: number of times arm A was pulled. nb: number of times arm B was pulled
na <- arm_a_successes + arm_a_failures
nb <- arm_b_successes + arm_b_failures
# estimated posterior probability of arms a and b
# [] posterior prob for arm a to be successful for each timepoint
pa_est <- arm_a_successes / na
pb_est <- arm_b_successes / nb
# waldScore for confidence interval, is a way to analyze categorical data underlying chi-squared distribution
# vectors, instead of summation
WaldScore <-
(pa_est - pb_est) / sqrt(pa_est * (1 - pa_est) / na + pb_est * (1 - pb_est) /nb)
# vector of rewards (history)
# expected reward we have from both arms at different sample sizes
# TODO: rename to expected reward
reward <- (na * pa + nb * pb) / c(1:n)
#
return(c(WaldScore[n], reward[n], APT_ban_UR, APT_w_UR))
}
#' Critical Value (Normal)
#'
#' Finds critical value using normal assumption on LLN
#' @param APT AP Test statistic
#' @param alpha TODO: what is this parameter?
#'
#' @return critical value
normal_cv <- function(APT, alpha){
m <- mean(APT)
v <- var(APT)
sd <- sqrt(v)
normal_rej <- qnorm(1-alpha)
cv <- normal_rej * sd + m
return(cv)
}
knitr::opts_chunk$set(echo = TRUE)
#' Critical Value (Normal)
#'
#' Finds critical value using normal assumption on LLN
#' @param APT AP Test statistic
#' @param alpha the customized significance level
#'
#' @return calculated critical value from normality assumption
normal_cv <- function(APT, alpha){
m <- mean(APT)
v <- var(APT)
sd <- sqrt(v)
normal_rej <- qnorm(1-alpha)
cv <- normal_rej * sd + m
return(cv)
}
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0 # effect size is 0: no difference between arms
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
N <- 15 # sample_size(total number of trials in one simulation)
B <- 5000 # number of simulations
c <- 0.1 # threshold
burn_in <- 2 # # of burnin trials
n <-  3 # batch_size
Ti <- N/n # num_batches
results <- array(dim = c(B, 4))
for (i in 1:B) {
results[i,] <-
TSPDD(
pa = p0,
pb = p1,
n = Ti,
c = c,
burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1, burnin=2 means there is one UR distribution before apply TSPost-Diff
batch = n
)
}
average_power = mean(results[, 1] > 1.96)
average_reward = mean(results[, 2])
APT1 = results[,3]
APT2 = results[,4]
#Histogram of APT1
hist(APT1, freq = FALSE)
# empirical and normal CV of APT1 is
em_cv <- empirical_cv(APT1,alpha)
paste('empirical CV is', em_cv)
norm_cv <- normal_cv(APT1,alpha)
paste('normal CV is', norm_cv)
# empirical cdf of APT1 is
F1 <- ecdf(APT1)
# empirical and normal FPR of APT1 is
paste('empirical FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
qnorm(1-alpha)
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0 # effect size is 0: no difference between arms
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
N <- 15 # sample_size(total number of trials in one simulation)
B <- 5000 # number of simulations
c <- 0.1 # threshold
burn_in <- 2 # # of burnin trials
n <-  3 # batch_size
Ti <- N/n # num_batches
results <- array(dim = c(B, 4))
for (i in 1:B) {
results[i,] <-
TSPDD(
pa = p0,
pb = p1,
n = Ti,
c = c,
burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1, burnin=2 means there is one UR distribution before apply TSPost-Diff
batch = n
)
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT1 = results[,3]
APT2 = results[,4]
#Histogram of APT1
hist(APT1, freq = FALSE)
# empirical and normal CV of APT1 is
em_cv <- empirical_cv(APT1,alpha)
paste('empirical CV is', em_cv)
norm_cv <- normal_cv(APT1,alpha)
paste('normal CV is', norm_cv)
# empirical cdf of APT1 is
F1 <- ecdf(APT1)
# empirical and normal FPR of APT1 is
paste('empirical FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
knitr::opts_chunk$set(echo = TRUE)
m <- mean(APT_w_UR)
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.2
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
for (i in 1:B) {
results[i,] <-
TSPDD(
pa = p0,
pb = p1,
n = Ti,
c = c,
burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
batch = n
)
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_w_UR
hist(APT_w_UR, freq = FALSE)
F1 <- ecdf(APT_w_UR)
paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
m <- mean(APT_w_UR)
sd <- sqrt(var(APT_w_UR))
hist((APT_w_UR-m)/sd)
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0 # effect size is 0: no difference between arms
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
N <- 15 # sample_size(total number of trials in one simulation)
B <- 5000 # number of simulations
c <- 0.1 # threshold
burn_in <- 2 # # of burnin trials
n <-  3 # batch_size
Ti <- N/n # num_batches
results <- array(dim = c(B, 4))
for (i in 1:B) {
results[i,] <-
TSPDD(
pa = p0,
pb = p1,
n = Ti,
c = c,
burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1, burnin=2 means there is one UR distribution before apply TSPost-Diff
batch = n
)
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_ban_UR
hist(APT_ban_UR, freq = FALSE)
# empirical and normal CV of APT_ban_UR is
em_cv <- empirical_cv(APT_ban_UR,alpha)
paste('empirical CV is', em_cv)
norm_cv <- normal_cv(APT_ban_UR,alpha)
paste('normal CV is', norm_cv)
# empirical cdf of APT_ban_UR is
F1 <- ecdf(APT_ban_UR)
# empirical and normal FPR of APT_ban_UR is
paste('empirical FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# house level data
d <- read.table(url("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat"), header=T, sep=",")
# deal with zeros, select what we want, makke a fips variable to match on
d <- d |>
mutate(activity = ifelse(activity==0, 0.1, activity)) |>
mutate(fips = stfips * 1000 + cntyfips) |>
dplyr::select(fips, state, county, floor, activity)
# county level data
cty <- read.table(url("http://www.stat.columbia.edu/~gelman/arm/examples/radon/cty.dat"), header = T, sep = ",")
cty <- cty |> mutate(fips = 1000 * stfips + ctfips) |> dplyr::select(fips, Uppm)
# filter to just be minnesota, join them and then select the variables of interest.
dmn <- d |>
filter(state=="MN") |>
dplyr::select(fips, county, floor, activity) |>
left_join(cty)
head(dmn)
dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull
dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull()
as.numeric(as.factor(dmn$county))
library(rstan)
dmn
y < log(dmn$activity)
library(rstan)
dmn
y <- log(dmn$activity)
x <- dmn$floor
u <- log(dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull())
N <- nrow(dmn)
J <- length(unique(dmn$county))
county <- as.numeric(as.factor(dmn$county))
stan_data <- list(y=y, x=x, u=u, N=N, J=J, county = county)
mod <- stan(stan_data, file = 'radon.stan')
library(rstan)
y <- log(dmn$activity)
x <- dmn$floor
u <- log(dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull())
N <- nrow(dmn)
J <- length(unique(dmn$county))
county <- as.numeric(as.factor(dmn$county))
stan_data <- list(y=y, x=x, u=u, N=N, J=J, county = county)
mod <- stan(stan_data, file = '/Users/haochensong/Desktop/Winter 2023/Applied Statistics II/applied-stats-2023/labs/radon.stan')
library(rstan)
y <- log(dmn$activity)
x <- dmn$floor
u <- log(dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull())
N <- nrow(dmn)
J <- length(unique(dmn$county))
county <- as.numeric(as.factor(dmn$county))
stan_data <- list(y=y, x=x, u=u, N=N, J=J, county = county)
mod <- stan(stan_data, file = "radon.stan")
library(rstan)
y <- log(dmn$activity)
x <- dmn$floor
u <- log(dmn |> group_by(county) |> slice(1) |> select(Uppm) |> pull())
N <- nrow(dmn)
J <- length(unique(dmn$county))
county <- as.numeric(as.factor(dmn$county))
stan_data <- list(y=y, x=x, u=u, N=N, J=J, county = county)
mod <- stan(data = stan_data, file = "radon.stan")
samps <- extract(mod)
names(samps)
alpha_hat <- apply(samps[["alpha"]],2, median)
alpha_lower <- apply(samps[["alpha"]],2, quantile, 0.025)
alpha_upper <- apply(samps[["alpha"]],2, quantile, 0.975)
alpha_df <- tibble(county - 1:J, median = alpha_hat, lower = alpha_lower, upper = alpha_upper, u = u)
samps <- extract(mod)
names(samps)
alpha_hat <- apply(samps[["alpha"]],2, median)
alpha_lower <- apply(samps[["alpha"]],2, quantile, 0.025)
alpha_upper <- apply(samps[["alpha"]],2, quantile, 0.975)
alpha_df <- tibble(county = 1:J, median = alpha_hat, lower = alpha_lower, upper = alpha_upper, u = u)
gamma0 <- median(samps[["gamma0"]])
gamma1 <- median(samps[["gamma1"]])
ggplot(alpha_df, aes(u,median)) +
geom_point()
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnomrl(alpha_2, sigma_y)
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnorml(alpha_2, sigma_y)
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnorm(alpha_2, sigma_y)
tibble(alpha = alpha_2, y = yrep_2) |>
ggplot(aes(y)) +
geom_desity(aes(fill = "predicted_y"), alpha = 0.6)+
geom_density(aes(alpha, fill = 'alpha', alpha = 0.6))
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnorm(alpha_2, sigma_y)
tibble(alpha = alpha_2, y = yrep_2) |>
ggplot(aes(y)) +
geom_density(aes(fill = "predicted_y"), alpha = 0.6)+
geom_density(aes(alpha, fill = 'alpha', alpha = 0.6))
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnorm(alpha_2, sigma_y)
tibble(alpha = alpha_2, y = yrep_2) |>
ggplot(aes(y)) +
geom_density(aes(fill = "predicted_y"), alpha = 0.6)+
geom_density(aes(alpha, fill = 'alpha'), alpha = 0.6))
alpha_2 <- samps[["alpha"]][,2]
sigma_y <- samps[["sigma_y"]]
yrep_2 <- rnorm(alpha_2, sigma_y)
tibble(alpha = alpha_2, y = yrep_2) |>
ggplot(aes(y)) +
geom_density(aes(fill = "predicted_y"), alpha = 0.6)+
geom_density(aes(alpha, fill = 'alpha'), alpha = 0.6)
