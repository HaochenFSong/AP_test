---
title: "AP test application"
output: pdf_document
date: "2022-08-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Section 1 Running TS-PostDiff (Code adapted from Tong)

```{r PostDiff, echo = TRUE}
#' TS-PostDiff - Allocation Probability Test
#'
#' Runs TS-PostDiff algorithm in two-armed case, for n batches, with given batch size and BURNIN period, to calculate an AP Test statistic for each arm

#' @param pa - actual prior probability of arm a (typically unknown in practice)
#' @param pb - actual prior probability of arm b (typically unknown in practice)
#' @param n - total number of trials in one simulation 
#' @param c - threshold of difference in expected reward between the two arms
#' @param burnin - number of trials that run only Uniform Random (UR) sampling, before applying the TS-PostDiff algorithm for all remaining trials, and updating posterior probability every batch (using beta distribution)
#' @param batch - number of trials before posterior probability is updated
#'
#' @return vector containing WaldScore[n], reward[n], APT_1, APT_2: the final waldscore, reward, and AP Test statistics for both arms

TSPDD <- function(pa, pb, n, c, burnin, batch) {

  # List: tracking successes and failures under each arm
  # ex. [0,1,1,1,2,3,3,4,5,6], final index value indicates total number of successes/failures
  arm_a_successes <- c(0)
  arm_a_failures <- c(0)
  arm_b_successes <- c(0)
  arm_b_failures <- c(0)
  
  # List: tracking only the current number of successes and failures (Value changes for each iteration)
  arm_a_s <- c(0)
  arm_a_f <- c(0)
  arm_b_s <- c(0)
  arm_b_f <- c(0)

  # History that tracks if there was success in either arm for each trial (Variable not used in final results)
  # ex. [1,0,1,1,0,1] 1 indicates success for either arm, 0 indicates failure for both arms
  abl <- c()
 
  APT_ban_UR = 0 # AP Test statistic, not including batches using UR sampling
  APT_w_UR = 0 # AP Test statistic, including batches using UR sampling
  
  # ---------- BURNIN PERIOD: use Uniform Random (UR) sampling only ----------
  
  for (i in 1:burnin) {

    draw_a <- runif(1) # Random probability of arm a being chosen
    draw_b <- runif(1) # Random probability of arm b being chosen

    # CASE 1: Arm a is chosen
    if (draw_a > draw_b) {
      # Arm a was successful  
      if (runif(1) < pa) {
        abl[i] <- 1
        arm_a_s <- arm_a_s + 1 # update arm a success count
      } 
      # Arm a was not successful
      else {       
        abl[i] <- 0
        arm_a_f <- arm_a_f + 1 # update arm a failure count
      }
    }

    # CASE 2: Arm b is chosen
    else {
      # Arm b was successful
      if (runif(1) < pb) {
        abl[i] <- 1
        arm_b_s <- arm_b_s + 1 # update arm b success count
      }
      # Arm b was not successful
      else {
        abl[i] <- 0
        arm_b_f <- arm_b_f + 1 # update arm b failure count
      }
    }

    # Tracking the build-up of total number of successes, failures, for arm a & b
    arm_a_successes[i] <- arm_a_s
    arm_a_failures[i] <- arm_a_f
    arm_b_successes[i] <- arm_b_s
    arm_b_failures[i] <- arm_b_f 
  }
  
  # ---------- AFTER BURNIN PERIOD: Use TS or UR depending on difference between posterior probabilities ----------
  
  for (i in (burnin + 1):n) {

    # Posterior probability updates happen by batch size (whenever m becomes 0, larger batch size -> slower updates)
    m = i %% batch 
    
    # Draw from Beta(alpha, beta) distribution, using only the current success & failure counts 
    # alpha = current successes, beta = current failures of each arm

    # CASE 1: Posterior probability of arms a & b is updated
    if (m == 0) {
      draw_a <-
        rbeta(1, arm_a_successes[i - 1] + 1, arm_a_failures[i - 1] + 1) 
      draw_b <-
        rbeta(1, arm_b_successes[i - 1] + 1, arm_b_failures[i - 1] + 1) 
    }
    
    # CASE 2: Posterior probability of arms a & b is not updated
    else {
      draw_a <-
        rbeta(1, arm_a_successes[i - m] + 1, arm_a_failures[i - m] + 1) 
      draw_b <-
        rbeta(1, arm_b_successes[i - m] + 1, arm_b_failures[i - m] + 1)
        
    # Using index [i - m] shows we are using the posterior probabilities from the previous batch, and not our most current success/failure counts
    }

    # Use UR: If difference between posterior probabilities of arms is less than threshold value (expected difference in reward between arms)
    if (abs(draw_a - draw_b) < c) {
      draw_a <- runif(1) # Random probability of arm a being chosen
      draw_b <- runif(1) # Random probability of arm b being chosen
    } 
    
    # Use TS: If difference between posterior probabilities of arms crosses threshold value (expected difference in reward between arms)
    else {
      # CASE 1: Posterior probability of arms a & b is updated
      if (m == 0) {
        draw_a <-
          rbeta(1, arm_a_successes[i - 1] + 1, arm_a_failures[i - 1] + 1)
        draw_b <-
          rbeta(1, arm_b_successes[i - 1] + 1, arm_b_failures[i - 1] + 1)
      }
      # CASE 2: Posterior probability of arms a & b is not updated
      else {
        draw_a <-
          rbeta(1, arm_a_successes[i - m] + 1, arm_a_failures[i - m] + 1)
        draw_b <-
          rbeta(1, arm_b_successes[i - m] + 1, arm_b_failures[i - m] + 1)
      }
      
      # APT_{ban UR} statistic is updated (version without UR sampling trials, inner indentation level)
      if (m == 0) {
      # APT_{ban UR} only counting cases where arm b is chosen, "experimental" arm
        if (draw_b > draw_a) {
          APT_ban_UR = APT_ban_UR + 1
        }
      }
    }

    # APT_{w UR} statistic is updated (version including trials with UR sampling, outer indentation level)
    if (m == 0) {
    # APT_{w UR} only counting cases where arm b is chosen, "experimental" arm
      if (draw_b > draw_a) {
        APT_w_UR = APT_w_UR + 1
      }
    }

    # Update arm a & b history (abl), successes, and failures
    if (draw_a > draw_b) {
      if (runif(1) < pa) {
        abl[i] <- 1
        arm_a_s <- arm_a_s + 1 # update arm a success count
      } else {
        abl[i] <- 0
        arm_a_f <- arm_a_f + 1 # update arm a failure count
      }
    } else{
      if (runif(1) < pb) {
        abl[i] <- 1
        arm_b_s <- arm_b_s + 1 #update arm b success count
      } else {
        abl[i] <- 0
        arm_b_f <- arm_b_f + 1 #update arm b failure count
      }
    }  
    
    # Tracking the build-up of total number of successes, failures, for arm a & b, adding onto successes and failures from burnin period
    arm_a_successes[i] <- arm_a_s
    arm_a_failures[i] <- arm_a_f
    arm_b_successes[i] <- arm_b_s
    arm_b_failures[i] <- arm_b_f
  }

  # List: complete history of number of times arms a & b were chosen (final index, total number of times chosen for each)
  na <- arm_a_successes + arm_a_failures
  nb <- arm_b_successes + arm_b_failures

  # List: complete history of estimated posterior probability updates, for arm a & b success (final index, final posterior probability for each)
  pa_est <- arm_a_successes / na
  pb_est <- arm_b_successes / nb

  # Wald Score for confidence interval: for analyzing categorical data underlying chi-squared distribution vectors, instead of summation
  WaldScore <- (pa_est - pb_est) / sqrt(pa_est * (1 - pa_est) / na + pb_est * (1 - pb_est) / nb)
  
  # List: our expected reward, from both arms (number of times chosen * posterior success probability, for each arm)
  reward <- (na * pa + nb * pb) / c(1:n)
  # Lists out the average expected reward for each individual trial (final index, final expected reward value)
  
  return(c(WaldScore[n], reward[n], APT_ban_UR, APT_w_UR))
}
```

## Finding Critical Values, Using Normal Assumption on Law of Large Numbers:

```{r}
#' Critical Value (Normal)
#'
#' Finds critical value using normal assumption on LLN
#' @param APT - AP Test statistic (List of either APT_ban_UR/APT_w_UR results across different simulations)
#' @param alpha - Customized significance level
#' 
#' @return generated critical value from normality assumption

normal_cv <- function(APT, alpha) {
  m <- mean(APT) 
  v <- var(APT)
  sd <- sqrt(v)
  normal_rej <- qnorm(1 - alpha) #Rejection area defined (following normal distribution assumption)
  cv <- normal_rej * sd + m
  return(cv)
}
```

## Finding Critical Values, Using Empirical Assumption (5000 simulations)

```{r}
#' Critical Value (Empirical)
#'
#' Finds critical value using empirical assumption on LLN
#' @param APT - AP Test statistic (List of either APT_ban_UR/APT_w_UR results across different simulations)
#' @param alpha - Customized significance level
#' 
#' @return generated critical value from empirical simulations

empirical_cv <- function(APT, alpha) {
  cdf = 0
  i = 0 
  # Repeatedly add the empirical probabilities of each [i] value until cdf reaches 1 - significance level
  while (cdf <= 1- alpha) {
    p_i <-sum(APT == i)/length(APT)
    cdf <- sum(cdf, p_i)
    i = i + 1
  }
  return(i - 1) 
}
```

## Calculating FPR for case where effect size is 0, to find critical values for APT_{ban UR}
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0 # effect size is 0: no difference between arms
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
N <- 150 # sample_size(total number of trials in one simulation)
B <- 5000 # number of simulations
c <- 0.1 # threshold
burn_in <- 2 # # of burnin trials
n <-  3 # batch_size
Ti <- N/n # num_batches


# One output for each simulation, the four dimensions are waldscore, average reward, APT_ban_UR, and APT_w_UR
results <- array(dim = c(B, 4))

# number of simulations
for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = N,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1, burnin=2 means there is one UR distribution before apply TSPost-Diff
      batch = n
    ) 
}

average_power = mean(results[, 1] > qnorm(1-alpha)) # Find average power based on waldscore
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]

#Histogram of APT_ban_UR
hist(APT_ban_UR, freq = FALSE)

# Empirical and normal CV of APT_ban_UR is
em_cv <- empirical_cv(APT_ban_UR,alpha)
paste('empirical CV is', em_cv)
norm_cv <- normal_cv(APT_ban_UR,alpha)
paste('normal CV is', norm_cv)

# Empirical cdf of APT_ban_UR is
F1 <- ecdf(APT_ban_UR)

# Empirical and normal FPR of APT_ban_UR is
paste('empirical FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
```

## Calculating power when p2 -p1 = 0.05
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.05 # effect size is 0.05: difference between arm's rewards is 0.05
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = N,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]

#Histogram of APT_ban_UR
hist(APT_ban_UR, freq = FALSE)
F1 <- ecdf(APT_ban_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```
## Calculating power when p2 -p1 = 0.1
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.1
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_ban_UR
hist(APT_ban_UR, freq = FALSE)

F1 <- ecdf(APT_ban_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```

## Calculating power when p2 -p1 = 0.2
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.2
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_ban_UR
hist(APT_ban_UR, freq = FALSE)

F1 <- ecdf(APT_ban_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```
## Calculating FPR  for case where effect size is 0, to find critical values for APT_w_UR


```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2
results <- array(dim = c(B, 4))
for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_w_UR
hist(APT_w_UR, freq = FALSE)

# emipircal CV of APT_w_UR is
em_cv <- empirical_cv(APT_w_UR,alpha)
paste('empirical CV is', em_cv)
# normal CV of APT_w_UR is

norm_cv <- normal_cv(APT_w_UR,alpha)
paste('normal CV is', norm_cv)

# empirical cdf of APT_w_UR is

F1 <- ecdf(APT_w_UR)

paste('empircal FPR is', 1-F1(em_cv))
paste('normal FPR is', 1-F1(floor(norm_cv)))
```

## Calculating power when p2 -p1 = 0.05
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.05
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_w_UR
hist(APT_w_UR, freq = FALSE)

F1 <- ecdf(APT_w_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```
## Calculating power when p2 -p1 = 0.1
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.1
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    ) 
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_w_UR
hist(APT_w_UR, freq = FALSE)

F1 <- ecdf(APT_w_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```

## Calculating power when p2 -p1 = 0.2
```{r FPR, echo=TRUE}
set.seed(0)
#calculate power and reward in case p0 < p1
alpha <- 0.05
effect_size <- 0.2
p0 <- 0.5 - effect_size / 2
p1 <- 0.5 + effect_size / 2

for (i in 1:B) {
  results[i,] <-
    TSPDD(
      pa = p0,
      pb = p1,
      n = Ti,
      c = c,
      burnin = burn_in, #burnin=1 means no burnin needs to be larger than batch-1
      batch = n
    )
    )
}
average_power = mean(results[, 1] > qnorm(1-alpha))
average_reward = mean(results[, 2])
APT_ban_UR = results[,3]
APT_w_UR = results[,4]
#Histogram of APT_w_UR
hist(APT_w_UR, freq = FALSE)

F1 <- ecdf(APT_w_UR)

paste('empircal Power is', 1-F1(em_cv))
paste('normal Power is', 1-F1(floor(norm_cv)))
```

## check for normality:
```{r}
m <- mean(APT_w_UR)
sd <- sqrt(var(APT_w_UR))

hist((APT_w_UR-m)/sd)
```

